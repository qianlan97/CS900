{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress: 100%|██████████| 810000/810000 [00:52<00:00, 15445.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Result:\n",
      "SGX Layers: [2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 19, 20, 22, 24, 25, 26, 29, 32, 35, 38, 41, 44, 54, 57]\n",
      "GPU Layers: [1, 4, 11, 15, 17, 18, 21, 23, 27, 28, 30, 31, 33, 34, 36, 37, 39, 40, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 58, 59, 60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Read the table data from the file\n",
    "table_data = []\n",
    "with open(\"table data.txt\", \"r\") as file:\n",
    "    lines = file.readlines()[1:]  # Skip the header line\n",
    "    for line in lines:\n",
    "        layer_data = line.strip().split()\n",
    "        layer_number = int(layer_data[1])\n",
    "        layer_type = layer_data[2]\n",
    "        sdc_probability = float(layer_data[3])\n",
    "        data_size = int(layer_data[4])\n",
    "        total_extra_time = float(layer_data[5])\n",
    "        table_data.append((layer_number, layer_type, sdc_probability, data_size, total_extra_time))\n",
    "\n",
    "# Normalize the factors\n",
    "max_sdc_probability = max(data[2] for data in table_data)\n",
    "max_data_size = max(data[3] for data in table_data)\n",
    "min_total_extra_time = min(data[4] for data in table_data)\n",
    "max_total_extra_time = max(data[4] for data in table_data)\n",
    "\n",
    "normalized_data = []\n",
    "for layer_number, layer_type, sdc_probability, data_size, total_extra_time in table_data:\n",
    "    normalized_sdc_probability = sdc_probability / max_sdc_probability\n",
    "    normalized_data_size = data_size / max_data_size\n",
    "    normalized_total_extra_time = (total_extra_time - min_total_extra_time) / (max_total_extra_time - min_total_extra_time) * 2 - 1\n",
    "    \n",
    "    normalized_data.append((layer_number, layer_type, normalized_sdc_probability, normalized_data_size, normalized_total_extra_time))\n",
    "\n",
    "# Define the layer type weights\n",
    "layer_type_weights = {\n",
    "    \"Conv2d\": 0.9,\n",
    "    \"Linear\": 0.8,\n",
    "    \"BatchNorm2d\": 0.7,\n",
    "    \"ReLU\": 0.3,\n",
    "    \"MaxPool2d\": 0.1,\n",
    "    \"Dropout\": 0.0\n",
    "}\n",
    "\n",
    "# Define the objective function for parameter optimization\n",
    "def objective_function(params):\n",
    "    w1, w2, w3, w4 = params\n",
    "    alpha = 0.5\n",
    "    beta = 0.5\n",
    "    total_costs = []\n",
    "    for layer_number, layer_type, sdc_probability, data_size, total_extra_time in normalized_data:\n",
    "        layer_type_weight = layer_type_weights[layer_type]\n",
    "        total_cost = w1 * layer_type_weight + w2 * sdc_probability + w3 * data_size + w4 * total_extra_time\n",
    "        total_costs.append((layer_number, total_cost))\n",
    "\n",
    "    total_costs.sort(key=lambda x: x[1], reverse=True)\n",
    "    rho = 26 / 60\n",
    "    sgx_layers = [layer[0] for layer in total_costs[:int(rho * len(total_costs))]]\n",
    "    \n",
    "    sdc_total = sum(data[2] for data in table_data if data[0] not in sgx_layers)\n",
    "    et_total = sum(data[4] for data in table_data if data[0] not in sgx_layers)\n",
    "    \n",
    "    objective = alpha * sdc_total + beta * et_total\n",
    "    \n",
    "    return objective\n",
    "\n",
    "weight_values = np.linspace(0, 1, 30)\n",
    "\n",
    "best_params = None\n",
    "best_objective = float(\"inf\")\n",
    "\n",
    "total_iterations = len(weight_values) ** 4\n",
    "\n",
    "with tqdm(total=total_iterations, desc=\"Grid Search Progress\") as pbar:\n",
    "    for w1 in weight_values:\n",
    "        for w2 in weight_values:\n",
    "            for w3 in weight_values:\n",
    "                for w4 in weight_values:\n",
    "                    params = (w1, w2, w3, w4)\n",
    "                    objective = objective_function(params)\n",
    "                    if objective < best_objective:\n",
    "                        best_params = params\n",
    "                        best_objective = objective\n",
    "                    pbar.update(1)\n",
    "\n",
    "w1, w2, w3, w4 = best_params\n",
    "total_costs = []\n",
    "for layer_number, layer_type, sdc_probability, data_size, total_extra_time in normalized_data:\n",
    "    layer_type_weight = layer_type_weights[layer_type]\n",
    "    total_cost = w1 * layer_type_weight + w2 * sdc_probability + w3 * data_size + w4 * total_extra_time\n",
    "    total_costs.append((layer_number, total_cost))\n",
    "\n",
    "total_costs.sort(key=lambda x: x[1], reverse=True)\n",
    "rho = 26 / 60\n",
    "sgx_layers = [layer[0] for layer in total_costs[:int(rho * len(total_costs))]]\n",
    "gpu_layers = [layer[0] for layer in total_costs[int(rho * len(total_costs)):]]\n",
    "\n",
    "sgx_layers.sort()\n",
    "gpu_layers.sort()\n",
    "\n",
    "print(\"Partition Result:\")\n",
    "print(\"SGX Layers:\", sgx_layers)\n",
    "print(\"GPU Layers:\", gpu_layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
